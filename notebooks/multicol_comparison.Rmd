---
title:  "Effect of multi-collinearity"
author: "Omkar Shetty"
date:   "03/11/2020"
output: html_document
---

The objective is to understand the impact that correlated freatures have on predictions when used with different algorithms.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo= FALSE, message=FALSE, warning=FALSE}

library(data.table)
library(xgboost)
library(Metrics)
library(randomForest)
library(car)

```

## Setting Everything Up

Here we create a dataset for the model

```{r}
# Define a config

config <- list(
    N = 5000,        # total no of rows
    const = 30,      # constant for dependent variable
    par_a = 15,      # coefficiant for feature a
    par_b = 0.3,     # coefficient for feature b
    par_c = 8,       # coefficient for feature c
    train_n = 4000   # No of rows used for training the model
)

set.seed(123)

```

Creating a simulated dataset. This dataset will be used to build and test the different models.
For this initial example, all features are numeric and no data are missing. 

```{r}
ttl_dat <- data.table(feat_a = runif(n = config$N, min = 1, max = 10),
                      feat_b = runif(n = config$N, min = 1, max = 100),
                      feat_c = runif(n = config$N, min = 10, max = 20))

str(ttl_dat)

```

Define a correlated feature. 
We will try two options - 
  a. feature with a perfect correlation
  b. feature with a strong correlation

```{r}

# Defining the feature with a perfect correlation

ttl_dat[,feat_a_dup := feat_a ]
ttl_dat[,cor(feat_a, feat_a_dup)]

```

```{r}

# Defining the second feature with a strong correlation

ttl_dat[,feat_a_cor := feat_a + rnorm(n = config$N, mean = 0, sd = 1)]
ttl_dat[,cor(feat_a, feat_a_cor)]

```


Defining a dependent variable (to be predicted) - we start with a linear form

```{r}
ttl_dat[,y_act := config$const +
                  rnorm(n = config$N ,mean = config$par_a*feat_a, sd = 10) + 
                  rnorm(n = config$N ,mean = config$par_b*feat_b, sd = 10) + 
                  rnorm(n = config$N ,mean = config$par_c*feat_c, sd = 10) ]
 
```

Now that the dataset is ready, we move towards modelling - starting by splitting data into train and test

```{r}

train_id <- sample(x = config$N, size = config$train_n, replace = F)
train_dat <- ttl_dat[train_id]
test_dat <- ttl_dat[!train_id]

```


Starting the model build with some linear models - which are known to be susceptible to multicollinearity

```{r}
# Linear Model

lm_fit1 <- lm(formula = 'y_act ~ feat_a + feat_b + feat_c', data = train_dat)

summary(lm_fit1)

```

As is seen by the model summary, the linear model was really successful in recovering the coefficients (which is not surprising).
We also see the model performance on the test data.

```{r}

test_dat[,y_lm1_pred := predict(object = lm_fit1, newdata = test_dat)]
test_dat[,Metrics::rmse(actual = y_act, predicted = y_lm1_pred)]

```

Next we include one of the correlated features (the non perfect one) and repeat the same exercise.

```{r}
# Linear Model - Take 2

lm_fit2 <- lm(formula = 'y_act ~ feat_a + feat_b + feat_c + feat_a_cor', data = train_dat)
summary(lm_fit2)

```

In this instance, there is a minor change in the coefficient for feat_a, but the model performance as such (as seen by the R squared) doesnt really get impacted all that much.

Also checking the VIF to ensure multicollinearity.

```{r}
car::vif(lm_fit2)
```

We also verify using the predictions on the test dataset.

```{r}

test_dat[,y_lm2_pred := predict(object = lm_fit2, newdata = test_dat)]
test_dat[,Metrics::rmse(actual = y_act, predicted = y_lm2_pred)]

```

The RMSE is just a little worse than what it was earlier.

## Random Forests


```{r}
rf_fit1 = randomForest(y_act ~ feat_a + feat_b + feat_c , 
                       data=train_dat, 
                       ntree=100, 
                       mtry=2, 
                       importance=TRUE)

varImpPlot(rf_fit1)
```


```{r}
rf_fit2 = randomForest(y_act ~ feat_a + feat_b + feat_c + feat_a_cor, 
                       data=train_dat, 
                       ntree=100, 
                       mtry=2, 
                       importance=TRUE)

varImpPlot(rf_fit2)
```

## XGBoost Models

Build an xgboost model

```{r}

feats <- c('feat_a','feat_b','feat_c')

train_mat <- data.matrix(train_dat[,..feats,with = F])
test_mat <- data.matrix(test_dat[,..feats,with = F])

fit <- xgboost::xgboost(data = train_mat,
                        label = train_dat$y_act,
                        nrounds = 5000,
                        silent = T,
                        print_every_n = 200)

pred <- predict(fit, test_mat)

test_dat[,y_pred := pred]

Metrics::rmse(actual = test_dat$y_act,predicted = test_dat$y_pred)
xgboost::xgb.importance(model = fit)

```

So in case of a perfect correlation, the implementation of the xgboost algorithm ignores one of the features.
How about when the feature has a non-perfect correlation


```{r}

feats <- c('feat_a','feat_b','feat_c','feat_a_cor')

train_mat <- data.matrix(train_dat[,..feats,with = F])
test_mat <- data.matrix(test_dat[,..feats,with = F])

fit <- xgboost::xgboost(data = train_mat,
                        label = train_dat$y_act,
                        nrounds = 5000,
                        silent = T,
                        print_every_n = 200)

pred <- predict(fit, test_mat)

test_dat[,y_pred := pred]

Metrics::rmse(actual = test_dat$y_act,predicted = test_dat$y_pred)
xgboost::xgb.plot.importance(xgb.importance(model = fit))

```

